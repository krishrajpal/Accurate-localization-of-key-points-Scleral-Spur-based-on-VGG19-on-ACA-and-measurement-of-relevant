!git clone https://github.com/krishrajpal/Accurate-localization-of-key-points-Scleral-Spur-based-on-VGG19-on-ACA-and-measurement-of-relevant.git Project


import numpy as np
# Load your data
x_train = np.load('npy/x_train.npy')  # Assuming you have numpy arrays saved as 'train_data.npy'
y_train = np.load('npy/y_train.npy')  # Assuming you have numpy arrays saved as 'train_labels.npy'
x_test = np.load('npy/x_test.npy')  # Assuming you have numpy arrays saved as 'test_data.npy'
y_test = np.load('npy/y_test.npy')


import tensorflow as tf
from tensorflow.keras import layers, models

# Define your VGG model
def vgg_model():
    model = models.Sequential()
    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(248, 248, 3)))
    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))
    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))

    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))
    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))
    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))

    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))
    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))
    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))
    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))

    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))
    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))
    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))
    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))

    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))
    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))
    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))
    model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))

    model.add(layers.Flatten())
    model.add(layers.Dense(4096, activation='relu'))
    model.add(layers.Dense(4096, activation='relu'))
    model.add(layers.Dense(2))  # Output layer for x, y coordinates

    return model

# Create an instance of the model
model = vgg_model()



model.compile(optimizer='adam', loss='mse')  # MSE loss for coordinate prediction


model.summary()


# Train the model
# model.fit(train_images, train_coordinates, epochs=10, batch_size=32, validation_data=(val_images, val_coordinates))
history = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_data=(x_test, y_test))
# Evaluate the model
# loss = model.evaluate(test_images, test_coordinates)
# print("Test Loss:", loss)


from sklearn.metrics import mean_squared_error, mean_absolute_error
# Predict on the test data
y_pred = model.predict(x_test)
# Calculate mean squared error
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)
# Calculate mean absolute error
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error:", mae)


y_pred


y_test


from matplotlib import pyplot
# plot learning curves
pyplot.title('Learning Curves')
pyplot.xlabel('Epoch')
pyplot.ylabel('Cross Entropy')
pyplot.plot(history.history['loss'], label='train')
# pyplot.plot(history.history['val_loss'], label='val')
pyplot.legend()
pyplot.show()


history.history['loss']



